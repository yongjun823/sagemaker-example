{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Host a Keras Sequential Model\n",
    "\n",
    "This notebook shows how to train and host a Keras Sequential model on SageMaker. The model used for this notebook is a simple deep CNN that was extracted from [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is one of the most popular machine learning datasets. It consists of 60,000 32x32 images belonging to 10 different classes (6,000 images per class). Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "In this tutorial, we will train a deep CNN to recognize these images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the CIFAR-10 dataset\n",
    "Downloading the test and training data will take around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.cifar10_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the dataset to an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='/tmp/cifar10_data', key_prefix='data/DEMO-cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sagemaker_session.upload_data` will upload the CIFAR-10 dataset from this machine to a bucket named **sagemaker-{region}-{*your aws account number*}**, if you don't have this bucket yet, `sagemaker_session` will create it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete source code\n",
    "Here is the full source code for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look:\n",
    "\n",
    "### The model function\n",
    "This function constitutes the main difference between TensorFlow and Keras models on SageMaker; Keras models have a `keras_model_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model_fn(hyperparameters):\n",
    "    \"\"\"keras_model_fn receives hyperparameters from the training job and returns a compiled keras model.\n",
    "    The model will be transformed into a TensorFlow Estimator before training and it will be saved in a \n",
    "    TensorFlow Serving SavedModel at the end of training.\n",
    "\n",
    "    Args:\n",
    "        hyperparameters: The hyperparameters passed to the SageMaker TrainingJob that runs your TensorFlow \n",
    "                         training script.\n",
    "    Returns: A compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', name='inputs', input_shape=(HEIGHT, WIDTH, DEPTH)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    opt = RMSPropOptimizer(learning_rate=hyperparameters['learning_rate'], decay=hyperparameters['decay'])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function builds and returns a compiled Keras model.\n",
    "\n",
    "**Note:** The first layer is named `PREDICT_INPUTS`. This serves as a workaround for a known issue where TensorFlow does not recognize the default (or any custom) name for the first layer of Keras models. Furthermore, note that we are wrapping our model in a `tf.keras.Model` before returning it. This serves as a workaround for a known issue where a Sequential model cannot be directly converted into an Estimator. See [here](https://github.com/tensorflow/tensorflow/issues/20552) for more information about the issue.\n",
    "\n",
    "### Input functions\n",
    "These functions are similar to those required by any other model using the TensorFlow Estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(params):\n",
    "    # Notice that the input placeholder has the same input shape as the Keras model input\n",
    "    tensor = tf.placeholder(tf.float32, shape=[None, HEIGHT, WIDTH, DEPTH])\n",
    "    \n",
    "    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n",
    "    inputs = {INPUT_TENSOR_NAME: tensor}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "\n",
    "def train_input_fn(training_dir, params):\n",
    "    return _input(tf.estimator.ModeKeys.TRAIN,\n",
    "                    batch_size=BATCH_SIZE, data_dir=training_dir)\n",
    "\n",
    "\n",
    "def eval_input_fn(training_dir, params):\n",
    "    return _input(tf.estimator.ModeKeys.EVAL,\n",
    "                    batch_size=BATCH_SIZE, data_dir=training_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_` and `eval_` functions call the `_input` function which returns a properly processed and shuffled (for training) set of images and labels.\n",
    "\n",
    "## Create a training job using the SageMaker TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 23:28:30.799640 139920829364032 estimator.py:290] tensorflow py2 container will be deprecated soon.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-07 23:28:31 Starting - Starting the training job...\n",
      "2019-09-07 23:28:33 Starting - Launching requested ML instances......\n",
      "2019-09-07 23:29:34 Starting - Preparing the instances for training...\n",
      "2019-09-07 23:30:23 Downloading - Downloading input data...\n",
      "2019-09-07 23:30:51 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m2019-09-07 23:30:51,758 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:51,759 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:51,773 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,858 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,858 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,858 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,858 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,859 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,859 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:54,859 INFO - tf_container - invoking the user-provided keras_model_fn\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:55,111 INFO - tensorflow - Using the Keras model provided.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:56,804 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f66256acb10>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[31mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[31mallow_soft_placement: true\u001b[0m\n",
      "\u001b[31mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints', '_master': ''}\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:56,824 INFO - tensorflow - Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:56,825 INFO - tensorflow - Skip starting Tensorflow server as there is only one node in the cluster.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:56,941 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,520 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,520 INFO - tensorflow - Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u's3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,520 INFO - tensorflow - Warm-starting from: (u's3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/keras/keras_model.ckpt',)\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,520 INFO - tensorflow - Warm-starting variable: conv2d_2/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,743 INFO - tensorflow - Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:57,892 INFO - tensorflow - Warm-starting variable: conv2d_1/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,029 INFO - tensorflow - Warm-starting variable: dense/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,182 INFO - tensorflow - Warm-starting variable: dense/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,333 INFO - tensorflow - Warm-starting variable: inputs/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,511 INFO - tensorflow - Warm-starting variable: inputs/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,669 INFO - tensorflow - Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,813 INFO - tensorflow - Warm-starting variable: conv2d/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:58,957 INFO - tensorflow - Warm-starting variable: conv2d_1/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:59,105 INFO - tensorflow - Warm-starting variable: conv2d_2/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:59,248 INFO - tensorflow - Warm-starting variable: conv2d/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:59,508 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:30:59,995 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:00,975 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:00,984 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:01,477 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:09,964 INFO - tensorflow - loss = 2.3615746, step = 1\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:33,183 INFO - tensorflow - global_step/sec: 4.30687\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:33,183 INFO - tensorflow - loss = 2.019133, step = 101 (23.219 sec)\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:55,898 INFO - tensorflow - global_step/sec: 4.4022\u001b[0m\n",
      "\u001b[31m2019-09-07 23:31:55,899 INFO - tensorflow - loss = 1.9847966, step = 201 (22.716 sec)\u001b[0m\n",
      "\n",
      "2019-09-07 23:32:33 Uploading - Uploading generated training model\u001b[31m2019-09-07 23:32:18,525 INFO - tensorflow - Saving checkpoints for 300 into s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:19,772 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:19,898 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:19,916 INFO - tensorflow - Starting evaluation at 2019-09-07-23:32:19\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:19,997 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:20,062 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/model.ckpt-300\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:20,431 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:20,442 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:21,325 INFO - tensorflow - Evaluation [10/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:22,083 INFO - tensorflow - Evaluation [20/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:22,841 INFO - tensorflow - Evaluation [30/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:23,621 INFO - tensorflow - Evaluation [40/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:24,399 INFO - tensorflow - Evaluation [50/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:25,159 INFO - tensorflow - Evaluation [60/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:25,916 INFO - tensorflow - Evaluation [70/100]\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:26,572 INFO - tensorflow - Finished evaluation at 2019-09-07-23:32:26\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:26,572 INFO - tensorflow - Saving dict for global step 300: accuracy = 0.36145175, global_step = 300, loss = 1.742104\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:26,926 INFO - tensorflow - Saving 'checkpoint_path' summary for global step 300: s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/model.ckpt-300\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,199 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,300 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,301 INFO - tensorflow - Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,301 INFO - tensorflow - Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,301 INFO - tensorflow - Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,301 INFO - tensorflow - Signatures INCLUDED in export for Predict: ['serving_default']\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,301 INFO - tensorflow - Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,381 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/model.ckpt-300\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,659 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1046: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,659 INFO - tensorflow - Assets added to graph.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:27,659 INFO - tensorflow - No assets to write.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:28,394 INFO - tensorflow - SavedModel written to: s3://sagemaker-us-west-2-415365883687/sagemaker-tensorflow-2019-09-07-23-28-30-800/checkpoints/export/Servo/1567899147/saved_model.pb\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:28,571 INFO - tensorflow - Loss for final step: 1.863049.\u001b[0m\n",
      "\u001b[31m2019-09-07 23:32:28,826 INFO - tf_container - Downloaded saved model at /opt/ml/model/export/Servo/1567899147\u001b[0m\n",
      "\n",
      "2019-09-07 23:32:38 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='cifar10_cnn.py',\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       hyperparameters={'learning_rate': 1e-4, 'decay':1e-6},\n",
    "                       training_steps=300, evaluation_steps=100,\n",
    "                       train_instance_count=1, train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Keras models have a known issue and cannot be used for distributed (multi-instance) training. Keep `train_instance_count == 1` until the TensorFlow/Keras team support this feature. See [here](https://github.com/tensorflow/tensorflow/issues/14504) for more information about the issue.\n",
    "\n",
    "\n",
    "## Deploy the trained model\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 22:49:07.801043 139920829364032 model.py:103] The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', py_version='py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions\n",
    "Prediction is not the focus of this notebook, so to verify the endpoint's functionality, we'll simply generate random data in the correct shape and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': {'activation_5': {'dtype': 1,\n",
       "   'tensor_shape': {'dim': [{'size': 1}, {'size': 10}]},\n",
       "   'float_val': [0.003199951956048608,\n",
       "    0.21724414825439453,\n",
       "    0.0012326259166002274,\n",
       "    0.003975691739469767,\n",
       "    0.004360983148217201,\n",
       "    0.0010368996299803257,\n",
       "    0.40071144700050354,\n",
       "    0.037878673523664474,\n",
       "    0.0004917706828564405,\n",
       "    0.32986781001091003]}},\n",
       " 'model_spec': {'name': 'generic_model',\n",
       "  'version': {'value': 1567896439},\n",
       "  'signature_name': 'serving_default'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating fake prediction data\n",
    "import numpy as np\n",
    "data = np.random.randn(1, 32, 32, 3)\n",
    "\n",
    "# The inputs key 'inputs_input' matches the Keras InputLayer name\n",
    "predictor.predict({'inputs_input': data}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'learning_rate': ContinuousParameter(0.01, 0.2)}\n",
    "objective_metric_name = 'loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': 'loss = ([0-9\\\\.]+)'}]\n",
    "\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InProgress'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the SageMaker Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
